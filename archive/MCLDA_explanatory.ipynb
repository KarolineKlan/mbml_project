{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple-channel latent Dirichlet allocation Pyro Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE SCRIPT Preprocess_data.py ONE TIME\n",
    "# load data CaseRigshospitalet_optimized.parquet\n",
    "df = pd.read_parquet('data/CaseRigshospitalet_optimized_withDistance.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCLDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import ClippedAdam\n",
    "\n",
    "def model(x_tokens, d_tokens, num_groups, num_diag_tokens, num_demo_tokens):\n",
    "    \"\"\"\n",
    "    Multi-Channel LDA model:\n",
    "    - x_tokens: list of diagnosis token indices for each patient [list of [N_x_p]]\n",
    "    - d_tokens: list of demographic token indices for each patient [list of [N_d_p]]\n",
    "    \"\"\"\n",
    "    num_patients = len(x_tokens)\n",
    "\n",
    "    # Hyperparameters\n",
    "    alpha = torch.ones(num_groups)\n",
    "    beta_x = 0.1 * torch.ones(num_diag_tokens)\n",
    "    beta_d = 0.1 * torch.ones(num_demo_tokens)\n",
    "\n",
    "    # GROUP-LEVEL TOPIC DISTRIBUTIONS\n",
    "    with pyro.plate(\"health_groups\", num_groups):\n",
    "        phi_x = pyro.sample(\"phi_x\", dist.Dirichlet(beta_x))\n",
    "        phi_d = pyro.sample(\"phi_d\", dist.Dirichlet(beta_d))\n",
    "\n",
    "    # PATIENT LOOP\n",
    "    for p in pyro.plate(\"patients\", num_patients):\n",
    "\n",
    "        # Patient-specific group distribution\n",
    "        theta_p = pyro.sample(f\"theta_{p}\", dist.Dirichlet(alpha))\n",
    "\n",
    "        # Diagnosis tokens\n",
    "        for n in pyro.plate(f\"x_tokens_{p}\", len(x_tokens[p])):\n",
    "            z_x = pyro.sample(f\"z_x_{p}_{n}\", dist.Categorical(theta_p))\n",
    "            pyro.sample(f\"w_x_{p}_{n}\", dist.Categorical(phi_x[z_x]), obs=x_tokens[p][n])\n",
    "\n",
    "        # Demographic tokens\n",
    "        for n in pyro.plate(f\"d_tokens_{p}\", len(d_tokens[p])):\n",
    "            z_d = pyro.sample(f\"z_d_{p}_{n}\", dist.Categorical(theta_p))\n",
    "            pyro.sample(f\"w_d_{p}_{n}\", dist.Categorical(phi_d[z_d]), obs=d_tokens[p][n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input shapes:\n",
    "num_groups = 10\n",
    "num_diag_tokens = 500    # Vocabulary size for diagnosis codes\n",
    "num_demo_tokens = 50     # Vocabulary size for demographic tokens\n",
    "\n",
    "# Simulated patient token data (indices)\n",
    "x_tokens = [torch.tensor([12, 34, 101]), torch.tensor([2, 5])]  # 2 patients\n",
    "d_tokens = [torch.tensor([4, 1]), torch.tensor([0, 3, 2])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.infer.autoguide as autoguide\n",
    "import pyro.poutine as poutine\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import ClippedAdam\n",
    "import torch\n",
    "\n",
    "# Define your model here (use the MCLDA model I gave you earlier)\n",
    "\n",
    "# Example synthetic token data (replace with your real data)\n",
    "x_tokens = [torch.tensor([1, 5, 2]), torch.tensor([3, 4]), torch.tensor([7, 8, 1, 2])]\n",
    "d_tokens = [torch.tensor([0, 1]), torch.tensor([2]), torch.tensor([3, 4, 5])]\n",
    "\n",
    "# Constants\n",
    "num_groups = 5\n",
    "num_diag_tokens = 20\n",
    "num_demo_tokens = 10\n",
    "\n",
    "# Ensure obs tokens are of type long\n",
    "x_tokens = [tokens.long() for tokens in x_tokens]\n",
    "d_tokens = [tokens.long() for tokens in d_tokens]\n",
    "\n",
    "# Build a wrapped model for SVI (fixing args)\n",
    "def model_wrapper():\n",
    "    return model(x_tokens, d_tokens, num_groups, num_diag_tokens, num_demo_tokens)\n",
    "\n",
    "# Guide setup\n",
    "guide = autoguide.AutoGuideList(model_wrapper)\n",
    "guide.add(autoguide.AutoDelta(poutine.block(model_wrapper, expose=['phi_x', 'phi_d'])))\n",
    "for p in range(len(x_tokens)):\n",
    "    guide.add(autoguide.AutoDiagonalNormal(poutine.block(model_wrapper, expose=[f\"theta_{p}\"])))\n",
    "\n",
    "# Optimizer and ELBO\n",
    "optimizer = ClippedAdam({\"lr\": 0.01})\n",
    "elbo = Trace_ELBO()\n",
    "svi = SVI(model_wrapper, guide, optimizer, loss=elbo)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "for step in range(500):\n",
    "    loss = svi.step()\n",
    "    losses.append(loss)\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step} - ELBO: {loss:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
