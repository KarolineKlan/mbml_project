{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b623bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import ClippedAdam\n",
    "from pyro_mixture_model import MixtureModel, MixtureModelGuide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55bcd1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/CaseRigshospitalet_summed.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7541fe97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           name     dtype  unique  size (MB)\n",
      "0                    Patient ID  category  325666         30\n",
      "1           Aktionsdiagnosekode  category    8125          1\n",
      "2  totalDiagnoseKontaktVarighed   float32    7831          1\n",
      "3                antalKontakter     int64     142          3\n",
      "4                antalDiagnoser     int64      17          3\n",
      "5                         alder   Float64   13870          4\n",
      "6                        gender  category       4          0\n",
      "7                    civilStand  category       9          0\n",
      "8          distanceToHospitalKM   float64    1688          3\n",
      "Total size (excluding \"embedding\"): 50.16899013519287 MB\n"
     ]
    }
   ],
   "source": [
    "def summarize_columns(df):\n",
    "    excluded_column = \"embedding\"\n",
    "    columns_to_summarize = [c for c in df.columns if c != excluded_column]\n",
    "    \n",
    "    print(pd.DataFrame([\n",
    "        (\n",
    "            c,\n",
    "            df[c].dtype,\n",
    "            len(df[c].unique()),\n",
    "            df[c].memory_usage(deep=True) // (1024**2)\n",
    "        ) for c in columns_to_summarize\n",
    "    ], columns=['name', 'dtype', 'unique', 'size (MB)']))\n",
    "    \n",
    "    print('Total size (excluding \"embedding\"):', \n",
    "          df[columns_to_summarize].memory_usage(deep=True).sum() / 1024**2, 'MB')\n",
    "\n",
    "# Call the function\n",
    "summarize_columns(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c64ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_demographic_features(df):\n",
    "    # Create dummy variables for categorical features\n",
    "    df = pd.get_dummies(df, columns=[\"gender\", \"civilStand\"], drop_first=True)\n",
    "    \n",
    "    # Ensure all columns in demographic_columns are numeric\n",
    "    demographic_columns = [\"alder\", \"distanceToHospitalKM\"] + [col for col in df.columns if col.startswith(\"gender_\") or col.startswith(\"civilStand_\")]\n",
    "\n",
    "    # Check for non-numeric columns\n",
    "    for col in demographic_columns:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            print(f\"Column {col} is not numeric. Converting to numeric.\")\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")  # Convert to numeric, setting invalid values to NaN\n",
    "\n",
    "    # Handle missing values (e.g., fill NaN with 0)\n",
    "    df[demographic_columns] = df[demographic_columns].fillna(0)\n",
    "    \n",
    "    return df[demographic_columns].values.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa09c973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Convert list of lists to a 2D NumPy array\n",
    "embedding_length = len(df[\"embedding\"].iloc[0])  # Assuming all embeddings should have the same length\n",
    "df[\"embedding\"] = df[\"embedding\"].apply(lambda x: x if len(x) == embedding_length else [0.0] * embedding_length)\n",
    "embeddings = np.array(df[\"embedding\"].tolist(), dtype=np.float32)\n",
    "embedding_tensor = torch.from_numpy(embeddings)\n",
    "\n",
    "# Make vector for each row with alder, gender, civilStand, distanceToHospitalKM,\n",
    "demographic_data = torch.from_numpy(make_demographic_features(df))\n",
    "\n",
    "# 1. Convert target to numpy\n",
    "y_time = torch.from_numpy(df['totalDiagnoseKontaktVarighed'].values.astype(np.float32))\n",
    "y_count = torch.from_numpy(df[\"antalKontakter\"].values.astype(np.int16))\n",
    "\n",
    "# 2. Randomly select 1000 indices\n",
    "total_samples = 1000\n",
    "all_indices = np.arange(len(embedding_tensor))\n",
    "\n",
    "selected_indices = np.random.choice(all_indices, size=total_samples, replace=False)\n",
    "df_subset = df.iloc[selected_indices]\n",
    "\n",
    "# Select based on patient ID, can't have same patient in train and test\n",
    "unique_patient_ids = df_subset['Patient ID'].unique()\n",
    "\n",
    "train_patient_ids, test_patient_ids = train_test_split(\n",
    "    unique_patient_ids, test_size=0.2, random_state=42\n",
    ")\n",
    "train_mask = df_subset['Patient ID'].isin(train_patient_ids)\n",
    "test_mask = df_subset['Patient ID'].isin(test_patient_ids)\n",
    "\n",
    "# Use the mask to filter rows\n",
    "x_emb_train = embedding_tensor[selected_indices][train_mask.values]\n",
    "d_demo_train = demographic_data[selected_indices][train_mask.values]\n",
    "v_time_train = y_time[selected_indices][train_mask.values]\n",
    "a_count_train = y_count[selected_indices][train_mask.values]\n",
    "\n",
    "x_emb_test = embedding_tensor[selected_indices][test_mask.values]\n",
    "d_demo_test = demographic_data[selected_indices][test_mask.values]\n",
    "v_time_test = y_time[selected_indices][test_mask.values]\n",
    "a_count_test = y_count[selected_indices][test_mask.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d0ce4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : loss = 5442133063.051021\n",
      "Step 1000 : loss = 97970580.06409946\n",
      "Step 2000 : loss = 1153587331.6007795\n",
      "Step 3000 : loss = 767687.1374959529\n",
      "Step 4000 : loss = 152972324.3349263\n",
      "Step 5000 : loss = 70246080.5972772\n",
      "Step 6000 : loss = 842245.0994409717\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m : loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# To run:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m100000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# adjust steps\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# After training, inspect pyro.param values for cluster parameters and q_alpha\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(num_steps)\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.isnan(value).any():\n\u001b[32m     12\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m contains NaN values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m loss = \u001b[43msvi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_emb_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_demo_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_time_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_count_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step % \u001b[32m1000\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m : loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marie\\miniconda3\\envs\\mbml\\Lib\\site-packages\\pyro\\infer\\svi.py:153\u001b[39m, in \u001b[36mSVI.step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m params = \u001b[38;5;28mset\u001b[39m(\n\u001b[32m    148\u001b[39m     site[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m].unconstrained() \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m param_capture.trace.nodes.values()\n\u001b[32m    149\u001b[39m )\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# actually perform gradient steps\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# zero gradients\u001b[39;00m\n\u001b[32m    156\u001b[39m pyro.infer.util.zero_grads(params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marie\\miniconda3\\envs\\mbml\\Lib\\site-packages\\pyro\\optim\\optim.py:155\u001b[39m, in \u001b[36mPyroOptim.__call__\u001b[39m\u001b[34m(self, params, *args, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_objs[p].optimizer.step(*args, **kwargs)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptim_objs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marie\\miniconda3\\envs\\mbml\\Lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marie\\miniconda3\\envs\\mbml\\Lib\\site-packages\\pyro\\optim\\clipped_adam.py:92\u001b[39m, in \u001b[36mClippedAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m     89\u001b[39m exp_avg.mul_(beta1).add_(grad, alpha=\u001b[32m1\u001b[39m - beta1)\n\u001b[32m     90\u001b[39m exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m denom = \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.add_(group[\u001b[33m\"\u001b[39m\u001b[33meps\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     94\u001b[39m bias_correction1 = \u001b[32m1\u001b[39m - beta1 ** state[\u001b[33m\"\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     95\u001b[39m bias_correction2 = \u001b[32m1\u001b[39m - beta2 ** state[\u001b[33m\"\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "pyro.set_rng_seed(0)\n",
    "\n",
    "# Setup SVI\n",
    "optimizer = ClippedAdam({\"lr\": 1e-4})\n",
    "svi = SVI(MixtureModel, MixtureModelGuide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "# Example training loop\n",
    "def train(num_steps=100000):\n",
    "    for step in range(num_steps):\n",
    "        for name, value in pyro.get_param_store().items():\n",
    "            if torch.isnan(value).any():\n",
    "                print(f\"Parameter {name} contains NaN values: {value}\")\n",
    "        loss = svi.step(x_emb_train, d_demo_train, v_time_train, a_count_train)\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Step {step} : loss = {loss}\")\n",
    "\n",
    "# To run:\n",
    "train(100000)  # adjust steps\n",
    "# After training, inspect pyro.param values for cluster parameters and q_alpha\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
